# -*- coding: utf-8 -*-
"""train_sl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zYBHtgv7PThGVq6JiAlWL_eEhM0vuvtH
"""

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import models, transforms
from custom_dataset import CustomDataset
from ssl_model import SwAVModel
from contrastive_loss import contrastive_loss

def train_swav(model, train_loader, optimizer, scheduler, num_epochs=10):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for epoch in range(num_epochs):
        total_loss = 0.0  # Track the total loss for the epoch
        for batch_idx, images in enumerate(train_loader):
            optimizer.zero_grad()

            features, projections = model(images.to(device))

            contrastive_loss_value = contrastive_loss(projections)

            total_loss += contrastive_loss_value.item()
            contrastive_loss_value.backward()
            optimizer.step()
            scheduler.step()

            if batch_idx % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {contrastive_loss_value.item():.4f}")

        print(f"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(train_loader):.4f}")

if __name__ == "__main__":
    dataset_path = "data path"
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
    ])

    train_dataset = CustomDataset(root_dir=dataset_path, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)

    base_encoder = models.resnet50(pretrained=True)
    swav_model = SwAVModel(base_encoder)
    optimizer = optim.SGD(swav_model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0.0001)

    train_swav(swav_model, train_loader, optimizer, scheduler, num_epochs=10)